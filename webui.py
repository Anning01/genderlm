# -*- coding: utf-8 -*-
import gradio as gr
from PIL import Image
import torch
import os
import io
from transformers import AutoImageProcessor, AutoModelForImageClassification
from face_detector import FaceDetector

# æ¨¡å‹è·¯å¾„é…ç½®
LOCAL_MODEL_PATH = "./gender-classification-2"
HF_MODEL_NAME = "rizvandwiki/gender-classification-2"

# ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
if os.path.exists(LOCAL_MODEL_PATH):
    print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH}")
    model_path = LOCAL_MODEL_PATH
else:
    print(f"â¬‡ï¸  æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä» Hugging Face ä¸‹è½½: {HF_MODEL_NAME}")
    model_path = HF_MODEL_NAME

# åŠ è½½æ€§åˆ«è¯†åˆ«æ¨¡å‹
print("ğŸ”„ æ­£åœ¨åŠ è½½æ€§åˆ«è¯†åˆ«æ¨¡å‹...")
processor = AutoImageProcessor.from_pretrained(model_path)
model = AutoModelForImageClassification.from_pretrained(model_path)
print("âœ… æ€§åˆ«è¯†åˆ«æ¨¡å‹åŠ è½½å®Œæˆ!")

# åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹
print("ğŸ”„ æ­£åœ¨åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹...")
try:
    # ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼ (0.4) æé«˜æ£€æµ‹çµæ•åº¦
    face_detector = FaceDetector(det_thresh=0.4)
    print("âœ… äººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½å®Œæˆ!")
    FACE_DETECTION_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸  äººè„¸æ£€æµ‹æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("âš ï¸  å°†ä¸æ”¯æŒäººè„¸æ£€æµ‹åŠŸèƒ½")
    face_detector = None
    FACE_DETECTION_AVAILABLE = False


def predict_gender(image, use_face_detection=False, face_scale=1.2):
    """
    æ€§åˆ«è¯†åˆ«å‡½æ•°

    å‚æ•°:
        image: PIL Image å¯¹è±¡æˆ– numpy array
        use_face_detection: æ˜¯å¦ä½¿ç”¨äººè„¸æ£€æµ‹
        face_scale: äººè„¸è£å‰ªç¼©æ”¾æ¯”ä¾‹

    è¿”å›:
        (ç»“æœå­—å…¸, å¤„ç†åçš„å›¾ç‰‡, ä¿¡æ¯æ–‡æœ¬)
    """
    if image is None:
        return {"é”™è¯¯": 1.0}, None, "âŒ è¯·ä¸Šä¼ å›¾ç‰‡"

    info_lines = []
    processed_image = image

    try:
        # ç¡®ä¿å›¾ç‰‡æ˜¯ RGB æ ¼å¼
        if isinstance(image, Image.Image):
            image = image.convert("RGB")
        else:
            image = Image.fromarray(image).convert("RGB")

        original_size = image.size
        info_lines.append(f"ğŸ“ åŸå§‹å›¾ç‰‡å°ºå¯¸: {original_size[0]} x {original_size[1]}")

        # äººè„¸æ£€æµ‹å’Œè£å‰ª
        face_detected = False
        if use_face_detection and FACE_DETECTION_AVAILABLE:
            info_lines.append("ğŸ” æ­£åœ¨è¿›è¡Œäººè„¸æ£€æµ‹...")
            cropped_face = face_detector.detect_and_crop(image, use_bbox=True, scale=face_scale)

            if cropped_face:
                face_detected = True
                info_lines.append("âœ… æ£€æµ‹åˆ°äººè„¸ï¼Œå·²è‡ªåŠ¨è£å‰ª")
                info_lines.append(f"âœ‚ï¸  è£å‰ªåå°ºå¯¸: {cropped_face.size[0]} x {cropped_face.size[1]}")
                image = cropped_face
                processed_image = cropped_face
            else:
                info_lines.append("âš ï¸  æœªæ£€æµ‹åˆ°äººè„¸ï¼Œä½¿ç”¨åŸå§‹å›¾ç‰‡")
        elif use_face_detection and not FACE_DETECTION_AVAILABLE:
            info_lines.append("âš ï¸  äººè„¸æ£€æµ‹åŠŸèƒ½ä¸å¯ç”¨")

        # é¢„å¤„ç†
        inputs = processor(images=image, return_tensors="pt")

        # æ¨ç†
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits

        # è®¡ç®—æ¦‚ç‡
        probabilities = torch.nn.functional.softmax(logits, dim=-1)[0]

        # æ„å»ºç»“æœå­—å…¸
        results = {
            model.config.id2label[i]: float(probabilities[i].item())
            for i in range(len(probabilities))
        }

        # ç”Ÿæˆè¯¦ç»†ä¿¡æ¯
        predicted_label = max(results.items(), key=lambda x: x[1])
        info_lines.append(f"\nğŸ¯ è¯†åˆ«ç»“æœ: {predicted_label[0]}")
        info_lines.append(f"ğŸ“Š ç½®ä¿¡åº¦: {predicted_label[1]:.2%}")

        if face_detected:
            info_lines.append(f"ğŸ”§ äººè„¸è£å‰ªç¼©æ”¾: {face_scale}x")

        info_text = "\n".join(info_lines)

        return results, processed_image, info_text

    except Exception as e:
        error_msg = f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}"
        return {"é”™è¯¯": 1.0}, None, error_msg


def predict_batch(files, use_face_detection=False, face_scale=1.2):
    """
    æ‰¹é‡å›¾ç‰‡è¯†åˆ«

    å‚æ•°:
        files: æ–‡ä»¶åˆ—è¡¨
        use_face_detection: æ˜¯å¦ä½¿ç”¨äººè„¸æ£€æµ‹
        face_scale: äººè„¸è£å‰ªç¼©æ”¾æ¯”ä¾‹

    è¿”å›:
        ç»“æœæ–‡æœ¬
    """
    if not files or len(files) == 0:
        return "âŒ è¯·ä¸Šä¼ è‡³å°‘ä¸€å¼ å›¾ç‰‡"

    results_text = [f"ğŸ“¦ æ‰¹é‡å¤„ç† {len(files)} å¼ å›¾ç‰‡\n{'='*50}\n"]

    for idx, file in enumerate(files, 1):
        try:
            # è¯»å–å›¾ç‰‡
            if isinstance(file, str):
                image = Image.open(file).convert("RGB")
                filename = os.path.basename(file)
            else:
                image = Image.open(file.name).convert("RGB")
                filename = os.path.basename(file.name)

            # äººè„¸æ£€æµ‹
            face_info = ""
            if use_face_detection and FACE_DETECTION_AVAILABLE:
                cropped = face_detector.detect_and_crop(image, use_bbox=True, scale=face_scale)
                if cropped:
                    image = cropped
                    face_info = " [äººè„¸å·²è£å‰ª]"
                else:
                    face_info = " [æœªæ£€æµ‹åˆ°äººè„¸]"

            # é¢„æµ‹
            inputs = processor(images=image, return_tensors="pt")
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits

            probabilities = torch.nn.functional.softmax(logits, dim=-1)[0]
            predicted_id = logits.argmax(-1).item()
            label = model.config.id2label[predicted_id]
            confidence = probabilities[predicted_id].item()

            # æ·»åŠ ç»“æœ
            results_text.append(
                f"{idx}. {filename}{face_info}\n"
                f"   æ€§åˆ«: {label} | ç½®ä¿¡åº¦: {confidence:.2%}\n"
            )

        except Exception as e:
            results_text.append(f"{idx}. å¤„ç†å¤±è´¥: {str(e)}\n")

    results_text.append(f"\n{'='*50}\nâœ… æ‰¹é‡å¤„ç†å®Œæˆ!")
    return "".join(results_text)


# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(
    title="æ€§åˆ«è¯†åˆ«ç³»ç»Ÿ",
    theme=gr.themes.Soft(),
    css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        #title {
            text-align: center;
            color: #2563eb;
        }
        #description {
            text-align: center;
            color: #64748b;
        }
        .info-box {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
    """
) as demo:
    gr.Markdown(
        """
        # ğŸ­ å›¾åƒæ€§åˆ«è¯†åˆ«ç³»ç»Ÿ
        åŸºäº Vision Transformer (ViT) çš„äººç‰©æ€§åˆ«è¯†åˆ«æœåŠ¡ | æ”¯æŒäººè„¸æ™ºèƒ½æ£€æµ‹ä¸è£å‰ª
        """,
        elem_id="title"
    )

    face_detection_status = "âœ… äººè„¸æ£€æµ‹å¯ç”¨" if FACE_DETECTION_AVAILABLE else "âš ï¸ äººè„¸æ£€æµ‹ä¸å¯ç”¨"
    gr.Markdown(
        f"""
        ç³»ç»ŸçŠ¶æ€: æ€§åˆ«è¯†åˆ«æ¨¡å‹å·²åŠ è½½ | {face_detection_status}
        """,
        elem_id="description"
    )

    with gr.Tabs():
        # Tab 1: å•å›¾è¯†åˆ«
        with gr.TabItem("ğŸ–¼ï¸ å•å›¾è¯†åˆ«"):
            with gr.Row():
                with gr.Column(scale=1):
                    input_image = gr.Image(
                        label="ä¸Šä¼ å›¾ç‰‡",
                        type="pil",
                        sources=["upload", "clipboard", "webcam"],
                        height=400
                    )

                    with gr.Row():
                        use_face_det = gr.Checkbox(
                            label="å¯ç”¨äººè„¸æ£€æµ‹",
                            value=False,
                            interactive=FACE_DETECTION_AVAILABLE
                        )
                        face_scale_slider = gr.Slider(
                            minimum=1.0,
                            maximum=2.0,
                            value=1.2,
                            step=0.1,
                            label="äººè„¸è£å‰ªç¼©æ”¾æ¯”ä¾‹",
                            interactive=FACE_DETECTION_AVAILABLE
                        )

                    predict_btn = gr.Button(
                        "ğŸ” å¼€å§‹è¯†åˆ«",
                        variant="primary",
                        size="lg"
                    )

                    clear_btn = gr.Button(
                        "ğŸ—‘ï¸ æ¸…é™¤",
                        variant="secondary"
                    )

                with gr.Column(scale=1):
                    output_label = gr.Label(
                        label="è¯†åˆ«ç»“æœ",
                        num_top_classes=2,
                        show_label=True
                    )

                    processed_image = gr.Image(
                        label="å¤„ç†åçš„å›¾ç‰‡",
                        type="pil",
                        height=300
                    )

                    info_text = gr.Textbox(
                        label="å¤„ç†ä¿¡æ¯",
                        lines=8,
                        max_lines=15
                    )

            gr.Markdown(
                """
                ### ğŸ“Š åŠŸèƒ½è¯´æ˜
                - **åŸºç¡€è¯†åˆ«**: ç›´æ¥å¯¹ä¸Šä¼ çš„å›¾ç‰‡è¿›è¡Œæ€§åˆ«è¯†åˆ«
                - **äººè„¸æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹å¹¶è£å‰ªäººè„¸åŒºåŸŸï¼Œæé«˜è¯†åˆ«å‡†ç¡®åº¦
                - **ç¼©æ”¾è°ƒèŠ‚**: è°ƒæ•´äººè„¸è£å‰ªèŒƒå›´ï¼ˆ1.0=ç´§è´´äººè„¸ï¼Œ2.0=åŒ…å«æ›´å¤šèƒŒæ™¯ï¼‰

                æ¨¡å‹å‡†ç¡®ç‡: **99.1%**
                """
            )

        # Tab 2: æ‰¹é‡è¯†åˆ«
        with gr.TabItem("ğŸ“ æ‰¹é‡è¯†åˆ«"):
            with gr.Row():
                with gr.Column(scale=1):
                    batch_files = gr.File(
                        label="ä¸Šä¼ å¤šå¼ å›¾ç‰‡",
                        file_count="multiple",
                        file_types=["image"]
                    )

                    with gr.Row():
                        batch_use_face = gr.Checkbox(
                            label="å¯ç”¨äººè„¸æ£€æµ‹",
                            value=False,
                            interactive=FACE_DETECTION_AVAILABLE
                        )
                        batch_scale = gr.Slider(
                            minimum=1.0,
                            maximum=2.0,
                            value=1.2,
                            step=0.1,
                            label="äººè„¸è£å‰ªç¼©æ”¾æ¯”ä¾‹",
                            interactive=FACE_DETECTION_AVAILABLE
                        )

                    batch_btn = gr.Button(
                        "ğŸš€ æ‰¹é‡è¯†åˆ«",
                        variant="primary",
                        size="lg"
                    )

                with gr.Column(scale=1):
                    batch_results = gr.Textbox(
                        label="æ‰¹é‡è¯†åˆ«ç»“æœ",
                        lines=20,
                        max_lines=30
                    )

            gr.Markdown(
                """
                ### ğŸ“¦ æ‰¹é‡å¤„ç†è¯´æ˜
                - æ”¯æŒåŒæ—¶ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œæ‰¹é‡è¯†åˆ«
                - å¯é€‰æ‹©æ˜¯å¦å¯¹æ¯å¼ å›¾ç‰‡è¿›è¡Œäººè„¸æ£€æµ‹
                - ç»“æœä¼šæ˜¾ç¤ºæ¯å¼ å›¾ç‰‡çš„è¯†åˆ«ç»“æœå’Œç½®ä¿¡åº¦
                """
            )

    # ç¤ºä¾‹å›¾ç‰‡æç¤ºï¼ˆGradio 3.x ç‰ˆæœ¬å…¼å®¹ï¼‰
    gr.Markdown(
        """
        ---
        ### ğŸ“ ç¤ºä¾‹å›¾ç‰‡
        å¦‚æœå·²ä¸‹è½½æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹è·¯å¾„çš„ç¤ºä¾‹å›¾ç‰‡è¿›è¡Œæµ‹è¯•ï¼š
        - `gender-classification-2/images/female.jpg`
        - `gender-classification-2/images/male.jpg`

        ---
        ğŸ’¡ **æç¤º**:
        - ä¸ºè·å¾—æœ€ä½³æ•ˆæœï¼Œè¯·ä¸Šä¼ æ¸…æ™°çš„äººç‰©ç…§ç‰‡
        - å¯ç”¨äººè„¸æ£€æµ‹å¯ä»¥æé«˜è¯†åˆ«å‡†ç¡®åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ…å«èƒŒæ™¯çš„ç…§ç‰‡ä¸­
        - æ‰¹é‡å¤„ç†æ—¶ï¼Œå»ºè®®æ¯æ¬¡ä¸Šä¼ ä¸è¶…è¿‡50å¼ å›¾ç‰‡
        """
    )

    # äº‹ä»¶ç»‘å®š - å•å›¾è¯†åˆ«
    predict_btn.click(
        fn=predict_gender,
        inputs=[input_image, use_face_det, face_scale_slider],
        outputs=[output_label, processed_image, info_text]
    )

    clear_btn.click(
        fn=lambda: (None, None, None, ""),
        inputs=None,
        outputs=[input_image, output_label, processed_image, info_text]
    )

    # è‡ªåŠ¨é¢„æµ‹ï¼ˆå¯é€‰ï¼‰
    input_image.change(
        fn=predict_gender,
        inputs=[input_image, use_face_det, face_scale_slider],
        outputs=[output_label, processed_image, info_text]
    )

    # äº‹ä»¶ç»‘å®š - æ‰¹é‡è¯†åˆ«
    batch_btn.click(
        fn=predict_batch,
        inputs=[batch_files, batch_use_face, batch_scale],
        outputs=batch_results
    )


if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
